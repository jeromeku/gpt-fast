{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, LlamaForCausalLM\n",
    "import torch\n",
    "from custom_flop_counter import PerformanceCounterMode\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe725839b3444d8b73f834b7fd9897a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"/home/ubuntu/gpt-fast-dev/checkpoints/7B\"\n",
    "model: LlamaForCausalLM = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, device_map=\"auto\", torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
    ")\n",
    "input_ids = torch.randint(\n",
    "    0, model.config.vocab_size, (1, 16), dtype=torch.int64, device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with PerformanceCounterMode(display=False, depth=10) as flop_counter:\n",
    "    _ = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = model.num_parameters()\n",
    "model_dtype_size = next(model.parameters()).element_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_counts = flop_counter.get_memory_counts()\n",
    "keys = list(mem_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_head_key = keys[-1]\n",
    "lm_head_size = sum(v for v in mem_counts[lm_head_key].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_modules = dict(model.named_modules())\n",
    "model_children = dict(model.named_children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = model.model.num_parameters()\n",
    "model_dtypes = set([p.dtype for p in model.parameters()])\n",
    "model_size = sum([p.numel() * p.element_size() for p in model.model.parameters()])\n",
    "lm_head_dtype = next(model.lm_head.parameters()).dtype\n",
    "lm_head_size = model.lm_head.weight.numel() * model.lm_head.weight.element_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13476831232 = 13214687232 + 262144000\n"
     ]
    }
   ],
   "source": [
    "print(f\"{model_size + lm_head_size} = {model_size} + {lm_head_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_token_size = sum(\n",
    "    [p.numel() * p.element_size() for p in model.model.embed_tokens.parameters()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13312248320 = 13048949248 + 263299072\n",
      "13574392320 = 13048949248 + 263299072 + 262144000\n",
      "Global count: 13574392320\n"
     ]
    }
   ],
   "source": [
    "mem_counts = flop_counter.get_data_counts()\n",
    "\n",
    "\n",
    "def sum_counts(counts, key):\n",
    "    return sum(counts[key].values())\n",
    "\n",
    "\n",
    "model_count = sum_counts(mem_counts, \"LlamaForCausalLM.model\")\n",
    "lm_head_count = sum_counts(mem_counts, \"LlamaForCausalLM.lm_head\")\n",
    "total_count = sum_counts(mem_counts, \"Global\")\n",
    "print(f\"{model_count + lm_head_count} = {model_count} + {lm_head_count}\")\n",
    "print(\n",
    "    f\"{model_count + lm_head_count + embed_token_size} = {model_count} + {lm_head_count} + {embed_token_size}\"\n",
    ")\n",
    "\n",
    "print(f\"Global count: {total_count + embed_token_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lm head diff: 0.001155072GB\n",
      "Model diff: 0.096406016GB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Lm head diff: {(lm_head_count - lm_head_size)/1e9}GB\")\n",
    "print(f\"Model diff: {(model_count + embed_token_size - model_size)/1e9}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_params = dict(model.named_parameters())\n",
    "params = list(model.model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.476831231999999 = 13.214687232 + 0.262144\n"
     ]
    }
   ],
   "source": [
    "params_sum = sum([p.numel() * p.element_size() for p in params]) / 1e9\n",
    "lm_head_sum = (\n",
    "    sum([p.numel() * p.element_size() for p in model.lm_head.parameters()]) / 1e9\n",
    ")\n",
    "print(f\"{params_sum + lm_head_sum} = {params_sum} + {lm_head_sum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "module_names = defaultdict(int)\n",
    "for k in named_params.keys():\n",
    "    normalized_name = k.split(\".\")[-2]\n",
    "    module_names[normalized_name] += (\n",
    "        named_params[k].numel() * named_params[k].element_size()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_tokens: 0.262144GB\n",
      "q_proj: 1.073741824GB\n",
      "k_proj: 1.073741824GB\n",
      "v_proj: 1.073741824GB\n",
      "o_proj: 1.073741824GB\n",
      "gate_proj: 2.885681152GB\n",
      "up_proj: 2.885681152GB\n",
      "down_proj: 2.885681152GB\n",
      "input_layernorm: 0.000262144GB\n",
      "post_attention_layernorm: 0.000262144GB\n",
      "norm: 8.192e-06GB\n",
      "lm_head: 0.262144GB\n"
     ]
    }
   ],
   "source": [
    "for k, v in module_names.items():\n",
    "    print(f\"{k}: {v/1e9}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params_no_norm_embed = sum(\n",
    "    [\n",
    "        module_names[k]\n",
    "        for k in module_names.keys()\n",
    "        if not (\"norm\" in k or \"lm_head\" in k or \"embed_tokens\" in k)\n",
    "    ]\n",
    ")\n",
    "embed_params = sum(\n",
    "    [module_names[k] for k in module_names.keys() if \"embed_tokens\" in k]\n",
    ")\n",
    "norm_params = sum([module_names[k] for k in module_names.keys() if \"norm\" in k])\n",
    "lm_params = sum([module_names[k] for k in module_names.keys() if \"lm_head\" in k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.476831232GB = 12.952010752 + 0.262144 + 0.00053248 + 0.262144\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"{(model_params_no_norm_embed + lm_params + norm_params + embed_params)/1e9}GB = {model_params_no_norm_embed/1e9} + {lm_params/1e9} + {norm_params/1e9} + {embed_params/1e9}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_modules = dict(model.named_modules())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "454"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "flop_counter.flop_counts.keys()\n",
    "flop_mods = defaultdict(int)\n",
    "for k in flop_counter.flop_counts.keys():\n",
    "    if \".\" in k:\n",
    "        splat = k.split(\".\")\n",
    "        m = splat[-1]\n",
    "        if not m.isdigit():\n",
    "            flop_mods[m] += sum_counts(flop_counter.flop_counts, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['self_attn', 'q_proj', 'model', 'k_proj', 'v_proj', 'rotary_emb', 'o_proj', 'mlp', 'gate_proj', 'up_proj', 'down_proj', 'lm_head'])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flop_mods.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'embed_tokens': 262144000,\n",
       "             'q_proj': 1073741824,\n",
       "             'k_proj': 1073741824,\n",
       "             'v_proj': 1073741824,\n",
       "             'o_proj': 1073741824,\n",
       "             'gate_proj': 2885681152,\n",
       "             'up_proj': 2885681152,\n",
       "             'down_proj': 2885681152,\n",
       "             'input_layernorm': 262144,\n",
       "             'post_attention_layernorm': 262144,\n",
       "             'norm': 8192,\n",
       "             'lm_head': 262144000})"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
